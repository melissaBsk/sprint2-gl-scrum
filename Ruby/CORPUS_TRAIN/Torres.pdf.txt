Filename: ./CORPUS_TRAIN/Torres.pdf
Title: Summary Evaluation
Abstract: Abstract—We    study  a  new   content-based  method   for  automatically generated summary (peer) has to be compared
the  evaluation  of  text  summarization   systems   without  with one or more reference summaries (models). DUC used
human models which is used to produce system rankings.
The research is carried out using a new content-based         an interface called SEE to allow human judges to compare
                                                              a peer with a model. Thus, judges give a C  OVERAGE    score
evaluation framework called F RESA  to compute a variety of
divergences among probability distributions. We apply our     to each peer produced by a system and the ﬁnal system
comparison framework to various well-established content-basedC OVERAGE    score is the average of the COVERAGE  ’s scores
evaluation measures in text summarization such as OVERAGE  ,  asigned. These system’s C OVERAGE    scores can then be used
R ESPONSIVENESS ,  PYRAMIDS    and   ROUGE   studying  their
                                                              to rank summarization systems. In the case of query-focused
associations in various text summarization tasks including    summarization (e.g. when the summary should answer a
generic multi-document summarization in English and French,
focus-based multi-document summarization in English and       question or series of questions) a R ESPONSIVENESS     score
generic single-document summarization in French and Spanish.  is also assigned to each summary, which indicates howIndex Terms—Text summarization evaluation, content-based    responsive the summary is to the question(s).
evaluation measures, divergences.                                Because manual comparison of peer summaries with model
                                                              summaries is an arduous and costly process, a body ofresearch has been produced in the last decade on automatic
                     I. NTRODUCTION                           content-based evaluation procedures. Early studies used text
     EXT summarization evaluation has always been a
                                                              similarity measures such as cosine similarity (with or without
T    complex and controversial issue in computational         weighting schema) to compare peer and model summaries
linguistics. In the last decade, signiﬁcant advances have been
                                                              [5]. Various vocabulary overlap measures such as n-grams
made in this ﬁeld as well as various evaluation measures have overlap or longest common subsequence between peer and
been designed. Two evaluation campaigns have been led by
                                                              model have also been proposed [6], [7]. The B  LEU  machine
the U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from         translation evaluation measure [8] has also been tested in
1996 to 1998 under the auspices of the Tipster program [1],   summarization [9]. The DUC conferences adopted the R  OUGE
and the second one, entitled DUC (Document Understanding
                                                              package for content-based evaluation [10]. OUGE  implements
Conference) [2], was the main evaluation forum from 2000      a series of recall measures based on n-gram co-occurrence
until 2007. Nowadays, the Text Analysis Conference (TAC)
                                                              between a peer summary and a set of model summaries. These
[3] provides a forum for assessment of different information  measures are used to produce systems’ rank. It has been shown
access technologies including text summarization.
                                                              that system rankings, produced by some R    OUGE   measures
  Evaluation in text summarization can be extrinsic or        (e.g., ROUGE -2, which uses 2-grams), have a correlation with
intrinsic [4]. In an extrinsic evaluation, the summaries are  rankings produced using C  OVERAGE  .
assessed in the context of an speciﬁc task carried out by a
                                                                 In recent years the YRAMIDS   evaluation method [11] has
human or a machine. In an intrinsic evaluation, the summaries been introduced. It is based on the distribution of “content”
are evaluated in reference to some ideal model. SUMMAC
                                                              of a set of model summaries. Summary Content Units (SCUs)
was mainly extrinsic while DUC and TAC followed an            are ﬁrst identiﬁed in the model summaries, then each SCU
intrinsic evaluation paradigm. In an intrinsic evaluation, an
                                                              receives a weight which is the number of models containing
  Manuscript received June 8, 2010. Manuscript accepted for publication Julythe same unit. Peer SCUs are identiﬁed in the
25, 2010.                                                     peer, matched against model SCUs, and weighted accordingly.Juan-Manuel T´rres-Moreno is with  LIA/Universited’Avignon, The P YRAMIDS    score given to a peer is the ratio of the sum
France   and   Ecole   Polytechnique de    Montreal,  Canada  of the weights of its units and the sum of the weights of the
(juan-manuel.torres@univ-avignon.fr).
  Eric  SanJuan  is   with  LIA/Universite d’Avignon, France  best possible ideal summary with the same number of SCUs as
(eric.sanjuan@univ-avignon.fr).                               the peer. The PYRAMIDS    scores can be also used for ranking
  Horacio Saggion is with DTIC/UniversitaPompeu  Fabra,Spain
(horacio.saggion@upf.edu).                                    summarization systems. [11] showed that P  YRAMIDS    scores
  Iriada  Cunha  is with IULA/UniversitaPompeu  Fabra, Spain; produced reliable system rankings when multiple (4 or more)
LIA/Universite d’Avignon, France and Instituto de Ingenier a/UNAM, Mexico
(iria.dacunha@upf.edu).                                       models were used and that P YRAMIDS   rankings correlate with
  Patricia Velazquez-Morales is   with  VM     Labs,  France  rankings produced by R OUGE  -2 and ROUGE  -SU2 (i.e. OUGE
(patricia velazquez@yahoo.com).                               with skip 2-grams). However, this method requires the creation13                                               Polibits (42) 2010,Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan,
 and Patricia Velázquez-Moralesof models and the identiﬁcation, matching, and weighting non-random systems, no clear conclusion was reached on the
SCUs in both: models and peers.                          value of each of the studied measures.[12] evaluated the effectiveness of the Jensen-Shannon  Nowadays,   a  widespread   summarization  evaluation
(J S) [13] theoretic measure in predicting systems ranks framework is ROUGE  [14], which offers a set of statistics
in two summarization tasks: query-focused and update     that compare peer summaries with models. It countssummarization. They have shown that ranks produced       co-occurrences of n-grams in peer and models to derive a
by   PYRAMIDS   and  those  produced  by  J S  measure   score. There are several statistics depending on the usedcorrelate. However, they did not investigate the effect  n-grams and the text processing applied to the input texts
of the measure in summarization tasks such as generic    (e.g., lemmatization, stop-word removal).
multi-document  summarization  (DUC    2004   Task  2),
                                                           [15] proposed a method of evaluation based on the
biographical summarization (DUC 2004 Task 5), opinion    use of “distances” or divergences between two probability
summarization (TAC 2008 OS), and summarization in        distributions (the distribution of units in the automaticlanguages other than English.                            summary and the distribution of units in the model
   In this paper we present a series of experiments aimedsummary). They studied two different Information Theoretica better understanding of the value of the J S divergencemeasures of divergence: the Kullback-Leibler (KL) [16] and
for ranking summarization systems. We have carried out   Jensen-Shannon (J S) [13] divergences. KL computes the
experimentation with the proposed measure and we have
                                                         divergence between probability distributions P and Q in the
veriﬁed that in certain tasks (such as those studied by  following way:
[12]) there is a strong correlation among P  YRAMIDS  ,
                                                                                   1X          P w
 RESPONSIVENESS   and the J S divergence, but as we will             D KL(PjjQ) =       Pwlog 2             (1)
show in this paper, there are datasets in which the correlation                    2 w         Q w
is not so strong. We also present experiments in Spanish
                                                         While J S divergence is deﬁned as follows:
and French showing positive correlation between the J S
and R OUGE  which is the de facto evaluation measure used              1X            2P w               2Q w
                                                         D J SPjjQ) =       Pwlog 2         +Q lwg  2
in evaluation of non-English summarization. To the best of             2 w         Pw+ Q  w           Pw+ Q  w
our knowledge this is the more extensive set of experiments                                                 (2)interpreting the value of evaluation without human models. These measures can be applied to the distribution of units in
   The rest of the paper is organized in the following wasystem summaries P and reference summaries Q. The value
First in Section II we introduce related work in the area of
                                                         obtained may be used as a score for the system summary. The
content-based evaluation identifying the departing point method has been tested by [15] over the DUC 2002 corpus for
our inquiry; then in Section III we explain the methodolosingle and multi-document summarization tasks showing goodadopted in our work and the tools and resources used for correlation among divergence measures and both coverage and
experimentation. In Section IV we present the experimentsROUGE  rankings.
carried out together with the results. Section V discusses the
                                                           [12] went even further and, as in [5], they proposed to
results and Section VI concludes the paper and identiﬁes compare directly the distribution of words in full documents
work.                                                    with the distribution of words in automatic summaries toderive a content-based evaluation measure. They found a
                  II. RELATED  W ORK                     high correlation between rankings produced using modelsOne of the ﬁrst works to use content-based measures inand rankings produced without models. This last work is the
text summarization evaluation is due to [5], who presented anrting point for our inquiry into the value of measures thatevaluation framework to compare rankings of summarizationdo not rely on human models.
systems produced by recall and cosine-based measures. They
showed that there was weak correlation among rankings
                                                                          III. METHODOLOGY
produced by recall, but that content-based measures produce
rankings which were strongly correlated. This put forward  The followed methodology in this paper mirrors the one
                                                         adopted in past work (e.g. [5], [7], [12]). Given a particular
the idea of using directly the full document for comparison
purposes in text summarization evaluation. [6] presented aummarization task T, p data points to be summarized
                                                         with input material iI g1 (e.g. document(s), question(s),
set of evaluation measures based on the notion of vocabulary                   i=0          s▯1
overlap including n-gram overlap, cosine similarity, and topic(s)), s peer summaries fSUi;kgk=0 for input i, and
longest common subsequence, and they applied them to     m model summaries fMODEL    i;jj=0  for input i, we willmulti-document summarization in English and Chinese.     compare rankings of the s peer summaries produced by various
However, they did not evaluate the performance of the    evaluation measures. Some measures that we use compare
                                                         summaries with n of the m models:
measures in different summarization tasks. [7] also compared
various evaluation measures based on vocabulary overlap.
Although these measures were able to separate random from        MEASURE (SUM        ;fMODEL     gn▯1)      (3)
                                                                            M      i;k         i;jj=0Polibits (42) 2010                                     14,                                                                                        Summary Evaluation with and without Referenceswhile other measures compare peers with all or some of the          3) Update-summarization task that consists of creating a
input material:                                                        summary out of a cluster of documents and a topic. Two0                           sub-tasks are considered here: A) an initial summary has
                  MEASURE (SUMM       i;k;Ii)              (4)         to be produced based on an initial set of documents and
         0
where I is some subset of input I . Thi values produced                topic; B) an update summary has to be produced from
by the measures for each summary SUM             are averaged          a different (but related) cluster assuming documents
                                             i;k                       used in A) are known. The English TAC’08 Update
for each system k = 0;:::;s ▯ 1 and these averages are
used to produce a ranking. Rankings are then compared                  Summarization dataset is used, which consists of 48
                                                                       topics with 20 documents each – 36,911 words.
using Spearman Rank correlation [17] which is used to
measure the degree of association between two variables             4) Opinion summarization where systems have to analyze
                                                                       a set of blog articles and summarize the opinions
whose values are used to rank objects. We have chosen
to use this correlation to compare directly results to those           about a target in the articles. The TAC’08 Opinion
presented in [12]. Computation of correlations is done using           Summarization in English    4 data set (taken from the
                                            1                          Blogs06 Text Collection) is used: 25 clusters and targets
the Statistics-RankCorrelation-0.12 package , which computes
the rank correlation between two vectors. We also veriﬁed              (i.e., target entity and questions) were used – 1,167,735
                                                                       words.
the good conformity of the results with the correlation test
of Kendall ▯ calculated with the statistical software R. The        5) Generic single-document summarization in Spanish
                                                                       using the Medicina Cl nica corpus, which is composed
two nonparametric tests of Spearman and Kendall do not
really stand out as the treatment of ex-æquo. The good                 of 50 medical articles in Spanish, each one with its
correspondence between the two tests shows that they do not            corresponding author abstract – 124,929 words.
                                                                    6) Generic single document summarization in French using
introduce bias in our analysis. Subsequently will mention only
the ▯ of Sperman more widely used in this ﬁeld.                        the “Canadien French Sociological Articles” corpus
                                                                       from the journal Perspectives interdisciplinaires sur le
                                                                                                   6
                                                                       travail et la sante (PISTES) . It contains 50 sociological
A. Tools                                                               articles in French, each one with its correspondingWe carry out experimentation using a new summarization               author abstract – 381,039 words.
evaluation framework: F   RESA   –FRamework for Evaluating          7) Generic multi-document-summarization in French using
                                                                                             7
Summaries Automatically–, which includes document-based                data from the RPM2 corpus [18], 20 different themes
summary     evaluation   measures    based   on   probabilities        consisting of 10 articles and 4 abstracts by reference
distribution . As in the R  OUGE   package, F  RESA   supports         thematic – 185,223 words.different n-grams and skip n-grams probability distributions.    For experimentation in the TAC and the DUC datasets we use
The F  RESA  environment can be used in the evaluation of
                                                                 directly the peer summaries produced by systems participating
summaries in English, French, Spanish and Catalan, and it        in the evaluations. For experimentation in Spanish and French
integrates ﬁltering and lemmatization in the treatment of
                                                                 (single and multi-document summarization) we have created
summaries and documents. It is developed in Perl and will        summaries at a similar ratio to those of reference using the
be made publicly available. We also use the R  OUGE   package    following systems:
[10] to compute various R  OUGE   statistics in new datasets.
                                                                    – ENERTEX [19], a summarizer based on a theory oftextual energy;
B. Summarization Tasks and Data Sets                                – CORTEX [20], a single-document sentence extractionWe have conducted our experimentation with the following             system for Spanish and French that combines various
summarization tasks and data sets:                                     statistical measures of relevance (angle between sentence
                                                                       and topic, various Hamming weights for sentences, etc.)
  1) Generic    multi-document-summarization      in   English
      (production of a short summary of a cluster of related           and applies an optimal decision algorithm for sentence
                                               3                       selection;
      documents) using data from DUC’04 , task 2: 50
      clusters, 10 documents each – 294,636 words.                  – SUMMTERM [21], a terminology-based summarizer that
                                                                       is used for summarization of medical articles and
  2) Focused-based summarization in English (production of
      a short focused multi-document summary focused on the            uses specialized terminology for scoring and ranking
                                                                       sentences;
      question “who is X?”, where X is a person’s name) using
      data from the DUC’04 task 5: 50 clusters, 10 documents        – REG [22], summarization system based on an greedy
      each plus a target person name – 284,440 words.                  algorithm;1http://search.cpan.org/ gene/Statistics-RankCorrelation-0.12/   4http://www.nist.gov/tac/data/index.html
  2FRESA  is available at: http://lia.univavignon.fr/ﬁleadmin/axes/TALNE///www.elsevier.es/revistas/ctl? f=7032&revistaid=2
Ressources.html                                                    6http://www.pistes.uqam.ca/
  3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html     7http://www-labs.sinequa.com/rpm215                                                 Polibits (42) 2010,Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan,
 and Patricia Velázquez-Morales– J S summarizer, a summarization system that scores      presented here we used uni-grams, 2-grams, and the skip
     and ranks sentences according to their Jensen-Shannon   2-grams with maximum skip distance of 4 (OUGE -1,divergence to the source document;                      ROUGE -2 and ROUGE -SU4). ROUGE  is used to compare
   – a lead-based summarization system that selects the lead a peer summary to a set of model summaries in oursentences of the document;                              framework (as indicated in equation 3).
   – a random-based summarization system that selects     – Jensen-Shannon divergence formula given in Equation 2sentences at random;                                    is implemented in ouRESA  package with the following
   – Open Text Summarizer [23], a multi-lingual summarizer   speciﬁcation (Equation 6) for the probability distributionbased on the frequency and                              of words w.
   – commercial systems: Word, SSSummarizer , Pertinence                                        CT
                 10                                                                       Pw =   w
     and Copernic .                                                                             N
                                                                             (      CS
                                                                                    NS    if w 2 S
C. Evaluation Measures                                                  Qw =     Cw+▯                      (6)
                                                                                N+▯▯B    otherwise
   The following measures derived from human assessment of
the content of the summaries are used in our experiments:    Where P is the probability distribution of words w in
                                                             text T and Q is the probability distribution of words w
   – COVERAGE   is understood as the degree to which one
                                                             in summary S; N is the number of words in text and
     peer summary conveys the same information as a model    summary N = N +N , B = 1:5jV j, C  T is the number
     summary [2]. COVERAGE  was used in DUC evaluations.                    T    S   S          w
                                                             of words in the text anw Cis the number of words in
     This measure is used as indicated in equation 3 using   the summary. For smoothing the summary’s probabilities
     human references or models.
   – RESPONSIVENESS   ranks summaries in a 5-point scale     we have used ▯ = 0:005. We have also implemented
                                                             other smoothing approaches (e.g. Good-Turing [24], that
     indicating how well the summary satisﬁed a given
     information need [2]. It is used in focused-based       uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2
                                                             package ) in F RESA , but we do not use them in
     summarization tasks. This measure is used as indicated
     in equation 4 since a human judges the summary          the experiments reported here. Following theOUGE
                                                             approach, in addition to word uni-grams we use 2-grams
     with respect to a given input “user need” (e.g., a
     question).ESPONSIVENESS   was used in DUC and TAC       and skip n-grams computing divergences such as J S
                                                             (using uni-grams) J2S (using 2-grams), 4 S (using the
     evaluations.                                            skip n-grams of OUGE  -SU4), and J S  which is an
   – PYRAMIDS  [11] is a content assessment measure which                                       M
                                                             average of the i S . J Ss measures are used to compare a
     compares content units in a peer summary to weighted    peer summary to its source document(s) in our framework
     content units in a set of model summaries. This
                                                             (as indicated in equation 4). In the case of summarization
     measure is used as indicated in equation 3 using human  of multiple documents, these are concatenated (in the
     references or models.YRAMIDS  is the adopted metric
     for content-based evaluation in the TAC evaluations.    given input order) to form a single input from which
                                                             probabilities are computed.
For DUC and TAC datasets the values of these measures areavailable and we used them directly. We used the following          IV. EXPERIMENTS AND   RESULTS
automatic evaluation measures in our experiments:
                                                          We ﬁrst replicated the experiments presented in [12] to
   – ROUGE   [14], which is a recall metric that takes iverify that our implementation of J S produced correlation
     account n-grams as units of content for comparing peer
                                                        results compatible with that work. We used the TAC’08
     and model summaries. The ROUGE formula speciﬁed in Update Summarization data set and computed J S and
     [10] is as follows:
                                                        R OUGE  measures for each peer summary. We produced
                                                        two system rankings (one for each measure), which wereROUGE-n(R;M) =            compared to rankings produced using the manuaYRAMIDS
       P        P                                       and R ESPONSIVENESS   scores. Spearman correlations were
         m 2 M    n▯ gram2P counmatch (n ▯ gram)
               P        P                          (5)  computed among the different rankings. The results are
                  m 2 M    count(n-gram)                presented in Table I. These results conﬁrm a high correlationwhere R is the summary to be evaluated, M is the set ofg P  YRAMIDS , R ESPONSIVENESS   and J S. We also
     model (human) summaries, count     is the number ofveriﬁed high correlation between J S and OUGE -2 (0:83
                                  match                 Spearman correlation, not shown in the table) in this task and
     common n-grams in m and P, and count is the number
     of n-grams in the model summaries. For the experimentsaset.
                                                          Then, we experimented with data from DUC’04, TAC’08
  8
   http://www.kryltech.com/summarizer.htm               Opinion Summarization pilot task as well as single and
  9http://www.pertinence.net
  10http://www.copernic.com/en/products/summarizer        1http://search.cpan.org/ bjoernw/Statistics-Smoothing-SGT-2.1.2/Polibits (42) 2010                                    16,                                                                                 Summary Evaluation with and without ReferencesTABLE I
 S PEARMAN CORRELATION OF CONTEN-BASED MEASURES ITAC’08     evaluation metrics that do not rely on human models but that
                U PDATESUMMARIZATION TASK                   compare summary content to input content directly [12]. Wehave some positive and some negative results regarding the
  Mesure   PYRAMIDS     p-value  RESPONSIVENESS    p-value  direct use of the full document in content-based evaluation.
 R OUGE-2     0.96    p < 0:005       0.92       p < 0:005    We have veriﬁed that in both generic muti-documentJ S        0.85    p < 0:005       0.74       p < 0:005  summarization    and   in   topic-based   multi-document
                                                            summarization  in  English  correlation among   measuresthat use human models (P    YRAMIDS  , R ESPONSIVENESS
                                                            and R OUGE  ) and a measure that does not use models
multi-document summarization in Spanish and French. In spite
of the fact that the experiments for French and Spanish corporaS divergence) is strong. We have found that correlation
use less data points (i.e., less summarizers per task) than among the same measures is weak for summarization of
                                                            biographical information and summarization of opinions in
for English, results are still quite signiﬁcant. For DUC’04,
we computed the J S measure for each peer summary in        blogs. We believe that in these cases content-based measures
                                                            should be considered, in addition to the input document, the
tasks 2 and 5 and we used J S, R   OUGE , COVERAGE    and
R ESPONSIVENESS   scores to produce systems’ rankings. The  summarization task (i.e. text-based representation, description)
various Spearman’s rank correlation values for DUC’04 are   to better assess the content of the peers [25], the task being a
                                                            determinant factor in the selection of content for the summary.
presented in Tables II (for task 2) and III (for task 5).
For task 2, we have veriﬁed a strong correlation between      Our multi-lingual experiments in generic single-document
                                                            summarization conﬁrm a strong correlation among the
J S and C  OVERAGE  . For task 5, the correlation between
J S and C  OVERAGE    is weak, and that between J S and     J S divergence and R  OUGE   measures. It is worth noting
                                                            that R OUGE   is in general the chosen framework for
R ESPONSIVENESS   is weak and negative.
  Although the Opinion Summarization (OS) task is a new     presenting content-based evaluation results in non-English
type of summarization task and its evaluation is a complicatedmmarization.
                                                              For the experiments in Spanish, we are conscious that we
issue, we have decided to compare J S rankings with those
obtained using YRAMIDS   and R ESPONSIVENESS   in TAC’08.   only have one model summary to compare with the peers.
                                                            Nevertheless, these models are the corresponding abstracts
Spearman’s correlation values are listed in Table IV. As it can
be seen, there is weak and negative correlation of J S with written by the authors. As the experiments in [26] show, the
                                                            professionals of a specialized domain (as, for example, the
both PYRAMIDS   and R ESPONSIVENESS  . Correlation between
PYRAMIDS   and R ESPONSIVENESS    rankings is high for this medical domain) adopt similar strategies to summarize their
task (0.71 Spearman’s correlation value).                   texts and they tend to choose roughly the same content chunks
                                                            for their summaries. Previous studies have shown that author
  For experimentation in mono-document summarization
in Spanish and French, we have run 11 multi-lingual         abstracts are able to reformulate content with ﬁdelity [27] and
                                                            these abstracts are ideal candidates for comparison purposes.
summarization systems; for experimentation in French, we
have run 12 systems. In both cases, we have produced        Because of this, the summary of the author of a medical article
                                                            can be taken as reference for summaries evaluation. It is worth
summaries at a compression rate close to the compression rate
of the authors’ provided abstracts. We have then computed J Soting that there is still debate on the number of models to be
and ROUGE  measures for each summary and we have averaged   used in summarization evaluation [28]. In the French corpus
                                                            PISTES, we suspect the situation is similar to the Spanish
the measure’s values for each system. These averages were
used to produce rankings per each measure. We computed      case.Spearman’s correlations for all pairs of rankings.
  Results are presented in Tables V, VI and VII. All results         VI. C ONCLUSIONS AND    FUTURE  W  ORK
show medium to strong correlation between the J S measures
                                                              This paper has presented a series of experiments in
and R OUGE  measures. However the J S measure based on
uni-grams has lower correlation than J Ss which use n-grams content-based measures that do not rely on the use of model
                                                            summaries for comparison purposes. We have carried out
of higher order. Note that table VII presents results for   extensive experimentation with different summarization tasks
generic multi-document summarization in French, in this
                                                            drawing a clearer picture of tasks where the measures could
case correlation scores are lower than correlation scores fobe applied. This paper makes the following contributions:
single-document summarization in French, a result which may
be expected given the diversity of input in multi-document    – We have shown that if we are only interested in ranking
                                                                summarization systems according to the content of their
summarization.
                                                                automatic summaries, there are tasks were models could
                                                                be subtituted by the full document in the computation of
                     V. D ISCUSSION
                                                                the J S measure obtaining reliable rankings. However,
  The departing point for our inquiry into text summarization   we have also found that the substitution of models
evaluation has been recent work on the use of content-based     by full-documents is not always advisable. We have17                                             Polibits (42) 2010,Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan,
 and Patricia Velázquez-MoralesTABLE II
                             SPEARMAN  ▯OF CONTENT -BASED MEASURES WITH C OVERAGE INDUC’04 T ASK 2Mesure     COVERAGE      p-value
                                               ROUGE -2      0.79     p < 0:0050
                                                 J S         0.68     p < 0:0025TABLE III
                                    SPEARMAN  ▯ OF CONTENT-BASED MEASURES IN DUC’04 T ASK 5Mesure    COVERAGE      p-value   RESPONSIVENESS     p-value
                                 R OUGE-2      0.78     p < 0:001        0.44         p < 0:05
                                    J S        0.40     p < 0:050        -0.18        p < 0:25TABLE IV
                                    SPEARMAN  ▯OF CONTENT -BASED MEASURES IN TAC’08 OS TASKMesure    P YRAMIDS    p-value   RESPONSIVENESS     p-value
                                    J S        -0.13    p < 0:25        -0.14        p < 0:25TABLE V
                    S PEARMAN ▯ OF CONTENT-BASED MEASURES WITH  ROUGE IN THEMedicina Cl nicaORPUS (SPANISH)Mesure    ROUGE -1    p-value   R OUGE-2     p-value  R OUGE -SU4     p-valueJ S         0.56     p < 0:100    0.46     p < 0:100      0.45      p < 0:200
                         J S2        0.88     p < 0:001    0.80     p < 0:002      0.81      p < 0:005
                         J S4        0.88     p < 0:001    0.80     p < 0:002      0.81      p < 0:005
                         J S         0.82     p < 0:005    0.71     p < 0:020      0.71      p < 0:010
                            Mfound weak correlation among different rankings in         a representation of the task/topic in the calculation ofcomplex summarization tasks such as the summarization      measures. To carry out these comparisons, however, we are
      of biographical information and the summarization of       dependent on the existence of references.opinions.                                                     FRESA  will also be used in the new question-answer task
   – We have also carried out large-scale experiments in
                                                                 campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
      Spanish and French which show positive medium to           qa.asp) for the evaluation of long answers. This task aimsstrong correlation among system’s ranks produced by        to answer a question by extraction and agglomeration of
      ROUGE    and divergence measures that do not use the       sentences in Wikipedia. This kind of task correspondsmodel summaries.                                           to those for which we have found a high correlation
   – We have also presented a new framework, F      RESA  , for
                                                                 among the measures J S and evaluation methods with
      the computation of measures based on J S divergence.       human intervention. Moreover, the J S calculation will beFollowing the R  OUGE   approach, F  RESA  package use     among the summaries produced and a representative set of
      word uni-grams, 2-grams and skip n-grams computing         relevant passages from Wikipedia. F   RESA   will be used todivergences. This framework will be available to the       compare three types of systems, although different tasks: the
      community for research purposes.
                                                                 multi-document summarizer guided by a query, the search
 Although we have made a number of contributions, this paper     systems targeted information (focused IR) and the questionleaves many open questions than need to be addressed. In        answering systems.order to verify correlation between R OUGE   and J S, in the
 short term we intend to extend our investigation to other
                                                                                     A CKNOWLEDGMENT
 languages such as Portuguese and Chinesse for which we
 have access to data and summarization technology. We also         We are grateful to the Programa Ramon y Cajal fromplan to apply F  RESA   to the rest of the DUC and TAC          Ministerio de Ciencia e Innovacion, Spain. This work issummarization tasks, by using several smoothing techniques.     partially supported by: a postdoctoral grant from the National
 As a novel idea, we contemplate the possibility of adapting     Program for Mobility of Research Human Resources (Nationalthe evaluation framework for the phrase compression task        Plan of Scientiﬁc Research, Development and Innovation
 [29], which, to our knowledge, does not have an efﬁcient        2008-2011, Ministerio de Ciencia e Innovacion, Spain); theevaluation measure. The main idea is to calculate J S from      research project CONACyT, number 82050, and the researchan automatically-compressed sentence taking the complete        project PAPIIT-DGAPA (Universidad Nacional Autonoma de
 sentence by reference. In the long term, we plan to incorporate Mexico), number IN403108.Polibits (42) 2010                                             18,                                                                                                 Summary Evaluation with and without ReferencesTABLE VISPEARMAN   ▯ OF CONTENT  BASED MEASURES WITH    R OUGE IN THE PISTES C  ORPUS  (FRENCH )Mesure     R OUGE -1     p-value    R OUGE -2    p-value    ROUGE -SU4       p-value
                            J S           0.70     p < 0:050      0.73      p < 0:05        0.73       p < 0:500
                            J S 2         0.93     p < 0:002      0.86      p < 0:01        0.86       p < 0:005J S 4         0.83     p < 0:020      0.76      p < 0:05        0.76       p < 0:050
                            J S M         0.88     p < 0:010      0.83      p < 0:02        0.83       p < 0:010TABLE VII
                           SPEARMAN   ▯ OF CONTENT -BASED MEASURES WITH    R OUGE IN THE RPM2 C  ORPUS  (FRENCH  )Measure     ROUGE  -1    p-value     ROUGE  -2    p-value   R OUGE -SU4     p-valueJ S           0.830     p < 0:002      0.660     p < 0:05       0.741      p < 0:01
                            J S2          0.800     p < 0:005      0.590     p < 0:05       0.680      p < 0:02
                            J S           0.750     p < 0:010      0.520     p < 0:10       0.620      p < 0:05
                               4
                            J SM          0.850     p < 0:002      0.640     p < 0:05       0.740      p < 0:01R EFERENCES                                  [18] C. de Loupy, M. Guegan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
                                                                             “A   French   Human    Reference  Corpus   for   multi-documents
                                                                             summarization and sentence compression,” in LREC’10, vol. 2,
 [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and               Malta, 2010, p. In press.
     B. Sundheim, “Summac: a text summarization evaluation,” Natural
     Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.              [19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
                                                                             of Associative Memories: performants applications of Enertex algorithm
 [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,        in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
     no. 6, pp. 1506–1520, 2007.
 [3] Proceedings of the Text Analysis ConferenceGaithesburg, Maryland,       861–871.
                                                                        [20] J.-M. Torres-Moreno, P. Velazquez-Morales, and J.-G. Meunier,
     USA: NIST, November 17-19 2008.                                         “Condenses de textes par des methodes numeriques,” in JADT’02, vol. 2,
 [4] K. Sparck Jones and J. Galliers, Evaluating Natural Language
     Processing Systems, An Analysis and Review, ser. Lecture Notes in       St Malo, France, 2002, pp. 723–734.
                                                                        [21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velazquez-Morales,
     Computer Science.  Springer, 1996, vol. 1083.                           “Automatic  summarization   using  terminological and   semantic
 [5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of        resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
     rankings produced by summarization evaluation measures,” in NAACL
                                                                        [22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
     Workshop on Automatic Summarization, 2000, pp. 69–78.                   applique au resume automatique de texte,” in JADT’10Rome, 2010,
 [6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation           p. In press.
     of Summaries in a Cross-lingual Environment using Content-based
     Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
                                                                             systems of automatic text summarization,” Automatic Documentation
 [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H.¸elebi, C     and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
     D. Liu, and E. Drabek, “Evaluation challenges in large-scale docume[24] C. D. Manning and H. Schutze, Foundations of Statistical Natural
     summarization,” in ACL’03, 2003, pp. 375–382.
                                                                             Language Processing.   Cambridge, Massachusetts: The MIT Press,
 [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method       1999.
     for automatic evaluation of machine translation,” in ACL’02, 2002, [25] K. Sparck Jones, “Automatic summarising: The state of the art,” IPM,
     311–318.
                                                                             vol. 43, no. 6, pp. 1449–1481, 2007.
 [9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation[26] I. da Cunha, L. Wanner, and M. T. Cabre, “Summarization of specialized
     Initiatives in Natural Language Processing. Budapest, Hungary: EACL,    discourse: The case of medical articles in spanish,” Terminology, vol. 13,
     14 April 2003.                                                          no. 2, pp. 249–286, 2007.[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of           [27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
     Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,        Student Research Workshop.     Toulouse, France: Association for
     M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.   Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in  [28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
     Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.             Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
     145–152.                                                                Singapore, August 2009, pp. 23–30.[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
     in Summarization without Human Models,” in Empirical Methods in         Sentence compression,” in Proceedings of the National Conference on
     Natural Language Processing, Singapore, August 2009, pp. 306–314.       Artiﬁcial IntelligenceMenlo Park, CA; Cambridge, MA; London;
                                                                             AAAI Press; MIT Press; 1999, 2000, pp. 703–710.
     [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
     Transactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
     N-gram Co-occurrence Statistics,” in HLT-NAACL.  Morristown, NJ,
     USA: Association for Computational Linguistics, 2003, pp. 71–78.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
     approach to automatic evaluation of summaries,” in HLT-NAACL,
     Morristown, USA, 2006, pp. 463–470.[16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann. of
     Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
     Sciences. McGraw-Hill, 1998.