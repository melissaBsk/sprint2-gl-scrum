Filename: ./CORPUS_TRAIN/Boudin-Torres-2006.pdf
Title: A Scalable MMR Approach to Sentence Scoring
Abstract: Abstract                         redundancy with previously read documents (his-
                                                    tory) has to be removed from the extract.
    We present S  MMR  , a scalable sentence
    scoring method for query-oriented up-             A natural way to go about update summarization
                                                    would be extracting temporal tags (dates, elapsed
    date summarization. Sentences are scored        times, temporal expressions...) (Mani and Wilson,
    thanks to a criterion combining query rele-
    vance and dissimilarity with already read       2000) or to automatically construct the timeline
                                                    from documents (Swan and Allan, 2000). These
    documents (history).   As the amount of
    data in history increases, non-redundancy       temporal marks could be used to focus extracts on
                                                    the most recently written facts. However, most re-
    is prioritized over query-relevance.  We        cently written facts are not necessarily new facts.
    show that S MMR   achieves promising re-
    sults on the DUC 2007 update corpus.            Machine Reading (MR) was used by (Hickl et
                                                    al., 2007) to construct knowledge representations
1   Introduction
                                                    from clusters of documents. Sentences contain-
                                                    ing “new” information (i.e. that could not be in-
Extensive experiments on query-oriented multi-      ferred by any previously considered document)
document summarization have been carried out
over the past few years. Most of the strategies     are selected to generate summary. However, this
                                                    highly efﬁcient approach (best system in DUC
to produce summaries are based on an extrac-
tion method, which identiﬁes salient textual seg-   2007 update) requires large linguistic resources.
                                                    (Witte et al., 2007) propose a rule-based system
ments, most often sentences, in documents. Sen-     based on fuzzy coreference cluster graphs. Again,
tences containing the most salient concepts are se-
lected, ordered and assembled according to their    this approach requires to manually write the sen-
                                                    tence ranking scheme. Several strategies remain-
relevance to produce summaries (also called ex-
tracts) (Mani and Maybury, 1999).                   ing on post-processing redundancy removal tech-
                                                    niques have been suggested. Extracts constructed
  Recently emerged from the Document Under-         from history were used by (Boudin and Torres-
standing Conference (DUC) 2007 , update sum-
marization attempts to enhance summarization        Moreno, 2007) to minimize history’s redundancy.
                                                    (Lin et al., 2007) have proposed a modiﬁed Max-
when more information about knowledge acquired
by the user is available. It asks the following ques-mal Marginal Relevance (MMR) (Carbonell and
                                                    Goldstein, 1998) re-ranker during sentence selec-
tion: has the user already read documents on the    tion, constructing the summary by incrementally
topic? In the case of a positive answer, producing
an extract focusing on only new facts is of inter-  re-ranking sentences.
                                                      In this paper, we propose a scalable sentence
est. In this way, an important issue is introduced:
    c                                               scoring method for update summarization derived
    2008.    Licensed under the Creative Commons   from MMR. Motivated by the need for relevant
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense  (http://creativecommons.org/licenses/by-nc-sanovelty, candidate sentences are selected accord-
Som1 rights reserved.                               ing to a combined criterion of query relevance and
    Document Understanding Conferences are conducteddissimilarity with previously read sentences. The
since 2000 by the National Institute of Standards and Tech-
nology (NIST), http://www-nlpir.nist.gov            rest of the paper is organized as follows. Section 223Coling 2008: Companion volume – Posters and Demonstrations, pages 23–26
                                        Manchester, August 2008,introduces our proposed sentence scoring method    sentence s and the query Q:
and Section 3 presents experiments and evaluates                           X
                                                        JW  (s,Q) =   1  ·     max J W (q,m)    (1)
our approach.                                              e         |Q|       m∈S 0
                                                                           q∈Q
                                                           0
2   Method                                         where S is the term set of s in which the terms
                                                   m that already have maximized J W (q,m) are re-The underlying idea of our method is that as the   moved. The use of JWe smooths normalization and
                                                   misspelling errors. Each sentence s is scored using
number of sentences in the history increases, the
likelihood to have redundant information within    the linear combination:
                                                                                 ~
candidate sentences also increases. We propose       Sim (1,Q) = α · cosine(~,Q)
a scalable sentence scoring method derived from                         + (1 − α) · W  (s,Q)    (2)
MMR that, as the size of the history increases,                                       egives more importance to non-redundancy that to    where α = 0.7, optimally tuned on the past DUCs
query relevance. We deﬁne H to represent the pre-  data (2005 and 2006). The system produces a listviously read documents (history), Q to represent   of ranked sentences from which the summary is
the query and s the candidate sentence. The fol-   constructed by arranging the high scored sentences
                                                   until the desired size is reached.
lowing subsections formally deﬁne the similarity
measures and the scalable MMR scoring method.
                                                   2.2  A scalable MMR approachMMR re-ranking algorithm has been successfully
2.1  A query-oriented multi-document               used in query-oriented summarization (Ye et al.,
     summarizer                                    2005). It strives to reduce redundancy while main-taining query relevance in selected sentences. The
We have ﬁrst started by implementing a simple      summary is constructed incrementally from a list
summarizer for which the task is to produce query-
focused summaries from clusters of documents.      of ranked sentences, at each iteration the sentence
                                                   which maximizes MMR is chosen:
Each document is pre-processed: documents are
segmented into sentences, sentences are ﬁltered       MMR = argmax [ λ · Sim (s,Q)
                                                                 s∈S            1
(words which do not carry meaning are removed
such as functional words or common words) and                  − (1 − λ) · maxSim (s2s ) ]j     (3)
                                                                           sj∈E
normalized using a lemmas database (i.e. inﬂected
forms “go”, “goes”, “went”, “gone”... are replaced where S is the set of candidates sentences and E
by “go”). An N-dimensional term-space Γ, where     is the set of selected sentences. λ represents anN is the number of different terms found in the    interpolation coefﬁcient between sentence’s rele-
cluster, is constructed. Sentences are represented vance and non-redundancy. Sim (2,s ) js a nor-
                                                   malized Longest Common Substring (LCS) mea-
in Γ by vectors in which each component is the
term frequency within the sentence. Sentence scor- sure between sentences s and sj. Detecting sen-
                                                   tence rehearsals, LCS is well adapted for redun-
ing can be seen as a passage retrieval task in Infor-
mation Retrieval (IR). Each sentence s is scored bydancy removal.
computing a combination of two similarity mea-       We propose an interpretation of MMR to tacklesures between the sentence and the query. The ﬁrst the update summarization issue. Since Sim 1nd
measure is the well known cosine angle (Salton et  Sim 2re ranged in [0,1], they can be seen as prob-
                                                   abilities even though they are not. Just as rewriting
al., 1975) between the sentence and the query vec-
torial representations in Γ (denoted respectively ~(3) as (NR stands for Novelty Relevance):
     ~
and Q). The second similarity measure is based       NR = argmax [ λ · Sim (s1Q)
on the Jaro-Winkler distance (Winkler, 1999). The             s∈S
original Jaro-Winkler measure, denoted JW , uses
                                                         + (1 − λ) · (1 − s ∈HSim (s2s ))h]     (4)
the number of matching characters and transposi-                           h
tions to compute a similarity score between two    We can understand that (4) equates to an OR com-terms, giving more favourable ratings to terms thatbination. But as we are looking for a more intu-
match from the beginning. We have extended this    itive AND and since the similarities are indepen-measure to calculate the similarity between the    dent, we have to use the product combination. The24,scoring method deﬁned in (2) is modiﬁed into a      3. An update summary of documents in C, un-
double maximization criterion in which the best        der the assumption that the reader has alreadyranked sentence will be the most relevant to the       read documents in A and B.
query AND the most different to the sentences in
H.                                                Within a topic, the document clusters must be pro-
                                                  cessed in chronological order. Our system gener-S MMR  (s) = Sim 1s,Q)                           ates a summary for each cluster by arranging the
                                                high ranked sentences until the limit of 100 words
                                       f(H)
              · 1 − max Sim (s2s ) h         (5)  is reached.
                    sh∈H
                                                  3.2   Evaluation
Decreasing λ in (3) with the length of the sum-
                                                  Most existing automated evaluation methods work
mary was suggested by (Murray et al., 2005) and   by comparing the generated summaries to one or
successfully used in the DUC 2005 by (Hachey
et al., 2005), thereby emphasizing the relevance  more reference summaries (ideally, produced by
                                                  humans). To evaluate the quality of our generated
at the outset but increasingly prioritizing redun-                                         3
dancy removal as the process continues.    Sim-   summaries, we choose to use the R  OUGE    (Lin,
                                                  2004) evaluation toolkit, that has been found to be
ilarly, we propose to follow this assumption in   highly correlated with human judgments. ROUGE  -
SMMR    using a function denoted f that as the
                                                  N  is a n-gram recall measure calculated between
amount of data in history increases, prioritize noa candidate summary and a set of reference sum-
redundancy (f(H) → 0).
                                                  maries. In our experiments R OUGE  -1, ROUGE -2
                                                  and R OUGE  SU 4 will be computed.
3   ExperimentsThe method described in the previous section has  3.3   Results
been implemented and evaluated by using the       Table 1 reports the results obtained on the DUC
                        2
DUC 2007 update corpus . The following subsec-    2007 update data set for different sentence scor-
tions present details of the different experimentsing methods. cosine + J  W e stands for the scor-we have conducted.                                ing method deﬁned in (2) and NR improves it
                                                  with sentence re-ranking deﬁned in equation (4).
3.1  The DUC 2007 update corpus
                                                  S MMR   is the combined adaptation we have pro-
We used for our experiments the DUC 2007 up-      posed in (5). The function f(H) used in MMR   is
date competition data set. The corpus is composed the simple rational function , where H increases
                                                                             H
of 10 topics, with 25 documents per topic. The up-with the number of previous clusters (f(H) = 1
date task goal was to produce short (∼100 words)  for cluster A,1 for cluster B and for cluster C).
                                                                2                 3
multi-document update summaries of newswire ar-   This function allows to simply test the assumption
ticles under the assumption that the user has al- that non-redundancy have to be favoured as theready read a set of earlier articles. The purpose size of history grows. Baseline results are obtained
of each update summary will be to inform the      on summaries generated by taking the leading sen-
reader of new information about a particular topictences of the most recent documents of the cluster,Given a DUC topic and its 3 document clusters: A  up to 100 words (ofﬁcial baseline of DUC). The
(10 documents), B (8 documents) and C (7 doc-     table also lists the three top performing systems atuments), the task is to create from the documents DUC 2007 and the lowest scored human reference.
three brief, ﬂuent summaries that contribute to sat- As we can see from these results, SMMR   out-isfying the information need expressed in the topiperforms the other sentence scoring methods. By
statement.                                        ways of comparison our system would have been1. A summary of documents in cluster A.         ranked second at the DUC 2007 update competi-
                                                  tion. Moreover, no post-processing was applied to
                                                  the selected sentences leaving an important margin
  2. An update summary of documents in B, un-
     der the assumption that the reader has alreadyf progress. Another interesting result is the high
                                                  performance of the non-update speciﬁc method
     read documents in A.
   2                                              (cosine + J We) that could be due to the small size
   More information about the DUC 2007 corpus is avai3-
able at http://duc.nist.gov/.                         ROUGE is available at http://haydn.isi.edu/ROUGE/.25,of the corpus (little redundancy between clusters).Hachey, B., G. Murray, and D. Reitter. 2005. The
                                                     Embra System at DUC 2005: Query-oriented Multi-
                                                     document Summarization with a Very Large Latent
                                                     Semantic Space. In Document Understanding Con-
                 ROUGE -1   ROUGE -2   ROUGE -SU4
                                                     ference (DUC).
 Baseline        0.26232    0.04543     0.08247
 3rd system      0.35715    0.09622     0.13245    Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC’s
 2nd system      0.36965    0.09851     0.13509      GISTexter at DUC 2007: Machine Reading for Up-
                                                     date Summarization. In Document Understanding
 cosine + W e    0.35905    0.10161     0.13701      Conference (DUC).
 NR              0.36207    0.10042     0.13781
                                                   Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and
  SMMR           0.36323    0.10223     0.13886
 1st system      0.37032    0.11189     0.14306      S. Ye. 2007. NUS at DUC 2007: Using Evolu-
                                                     tionary Models of Text. In Document Understanding
 Worst human     0.40497    0.10511     0.14779      Conference (DUC).Lin, C.Y. 2004. Rouge: A Package for Automatic
Table 1: R OUGE  average recall scores computed      Evaluation of Summaries. In Workshop on Text Sum-
on the DUC 2007 update corpus.                       marization Branches Out, pages 25–26.Mani, I. and M.T. Maybury. 1999. Advances in Auto-4   Discussion and Future Work                       matic Text Summarization. MIT Press.Mani, I. and G. Wilson. 2000. Robust temporal pro-
In this paper we have described S  MMR  , a scal-    cessing of news. In 38th Annual Meeting on Asso-
able sentence scoring method based on MMR that       ciation for Computational Linguistics, pages 69–76.
                                                     Association for Computational Linguistics Morris-
achieves very promising results. An important as-    town, NJ, USA.
pect of our sentence scoring method is that it does
not requires re-ranking nor linguistic knowledge,
                                                   Murray, G., S. Renals, and J. Carletta. 2005. Extractive
which makes it a simple and fast approach to the     Summarization of Meeting Recordings. In Ninth Eu-
issue of update summarization. It was pointed out    ropean Conference on Speech Communication and
                                                     Technology. ISCA.
at the DUC 2007 workshop that Question Answer-
ing and query-oriented summarization have been     Salton, G., A. Wong, and C. S. Yang. 1975. A vector
                                                     space model for automatic indexing. Communica-
converging on a common task. The value added
by summarization lies in the linguistic quality. Ap- tions of the ACM, 18(11):613–620.proaches mixing IR techniques are well suited for  Swan, R. and J. Allan. 2000. Automatic generation
query-oriented summarization but they require in-    of overview timelines. In 23rd annual international
                                                     ACM SIGIR conference on Research and develop-
tensive work for making the summary ﬂuent and        ment in information retrieval, pages 49–56.
coherent. Among the others, this is a point that we
think is worthy of further investigation.          Winkler, W. E. 1999. The state of record linkage and
                                                     current research problems. In Survey Methods Sec-Acknowledgments                                      tion, pages 73–79.This work was supported by the Agence Nationale    Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
                                                     ing Update Summaries for DUC 2007. In Document
de la Recherche, France, project RPM2.               Understanding Conference (DUC).Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS
                                                     at DUC 2005: Understanding documents via con-
References
                                                     cept links. In Document Understanding Conference
Boudin, F. and J.M. Torres-Moreno. 2007.  A Co-      (DUC).
  sine Maximization-Minimization approach for User-
  Oriented Multi-Document Update Summarization.
  In Recent Advances in Natural Language Processing
  (RANLP), pages 81–87.Carbonell, J. and J. Goldstein. 1998. The use of MMR,
  diversity-based reranking for reordering documents
  and producing summaries. In 21st annual interna-
  tional ACM SIGIR conference on Research and de-
  velopment in information retrieval, pages 335–336.